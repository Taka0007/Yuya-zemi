{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title タイタニック\n",
        "import csv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "import lightgbm as lgb\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split # データセット分割用\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pylab import rcParams\n",
        "from sklearn.metrics import confusion_matrix\n",
        "np.set_printoptions(suppress=True)#指数表記禁止\n",
        "\n",
        "import IPython\n",
        "def display(*dfs, head=True):\n",
        "    for df in dfs:\n",
        "        IPython.display.display(df.head() if head else df)\n",
        "\n",
        "# 特徴量重要度を棒グラフでプロットする関数 \n",
        "def plot_feature_importance(df): \n",
        "  n_features = len(df)                              # 特徴量数(説明変数の個数) \n",
        "  df_plot = df.sort_values('importance')            # df_importanceをプロット用に特徴量重要度を昇順ソート \n",
        "  f_importance_plot = df_plot['importance'].values  # 特徴量重要度の取得 \n",
        "  plt.barh(range(n_features), f_importance_plot, align='center') \n",
        "  cols_plot = df_plot['feature'].values             # 特徴量の取得 \n",
        "  plt.yticks(np.arange(n_features), cols_plot)      # x軸,y軸の値の設定\n",
        "  plt.xlabel('Feature importance')                  # x軸のタイトル\n",
        "  plt.ylabel('Feature')                             # y軸のタイトル\n",
        "\n",
        "\n",
        "#データよみこみ\n",
        "pretest  = pd.read_csv(filepath_or_buffer=\"/content/test.csv\")\n",
        "pretrain = pd.read_csv(filepath_or_buffer=\"/content/train.csv\")\n",
        "\n",
        "#扱いづらいデータを削除\n",
        "traindata = pretrain.drop(['Name','Cabin','Ticket','Fare','PassengerId','Parch'], axis=1)\n",
        "testdata  = pretest.drop(['Name','Cabin','Ticket','Fare','PassengerId','Parch'], axis=1)\n",
        "\n",
        "#欠損値処理\n",
        "np.nan_to_num(traindata['Age'], copy=False)\n",
        "np.nan_to_num(testdata['Age'], copy=False)\n",
        "\n",
        "#male or female\n",
        "SEX_le = LabelEncoder()\n",
        "traindata['Sex'] = SEX_le.fit_transform(traindata['Sex'])\n",
        "testdata['Sex']  = SEX_le.fit_transform(testdata['Sex'])\n",
        "\n",
        "#embarked\n",
        "eb_le = LabelEncoder()\n",
        "traindata['Embarked'] = eb_le.fit_transform(traindata['Embarked'])\n",
        "testdata['Embarked']  = eb_le.fit_transform(testdata['Embarked'])\n",
        "\n",
        "#Ageを整数型に変換\n",
        "traindata['Age'] = traindata['Age'].astype('int')\n",
        "testdata['Age']  = testdata['Age'].astype('int')\n",
        "\n",
        "\n",
        "# 説明変数,目的変数, テストケース分割\n",
        "X = traindata.drop('Survived',axis=1).values # 説明変数\n",
        "y = traindata['Survived'].values             # 目的変数\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.10, random_state=2,stratify=y) \n",
        "#↑学習用データ：テストデータ=8:2になるようにしている\n",
        "\n",
        "\n",
        "\n",
        "# 学習に使用するデータを設定\n",
        "putilgb_train = lgb.Dataset(X_train, y_train)\n",
        "putilgb_eval = lgb.Dataset(X_test, y_test, reference=putilgb_train) \n",
        "\n",
        "\n",
        "# LightGBM parameters\n",
        "params = {\n",
        "        'task': 'train',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'objective':'mape',\n",
        "        'metric': {'rmse'}, # 評価指標 : rsme(平均二乗誤差の平方根) \n",
        "        'learning_rate': 0.0001,\n",
        "        'num_iterations':50000,  \n",
        "        'num_leaves':70,\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# モデルの学習\n",
        "model = lgb.train(params,\n",
        "                  train_set=putilgb_train, # トレーニングデータの指定\n",
        "                  num_boost_round=1000,\n",
        "                  valid_sets=putilgb_eval, # 検証データの指定\n",
        "                  early_stopping_rounds=20,\n",
        "                  verbose_eval=0\n",
        "                  )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "pred = model.predict(testdata)\n",
        "\n",
        "for i in range(len(pred)):\n",
        "  \n",
        "  if pred[i] > 0.80:\n",
        "    pred[i] = 1\n",
        "\n",
        "  else:\n",
        "    pred[i] = 0\n",
        "\n",
        "\n",
        "pred   = pred.astype('int')\n",
        "\n",
        "pre_ans = np.stack([pretest[\"PassengerId\"], pred], 1)\n",
        "print(pre_ans)\n",
        "\n",
        "ans = pd.DataFrame(pre_ans, columns=['PassengerId', 'Survived'])\n",
        "#ans = ans.drop(ans.columns[[0]] , axis=1)\n",
        "ans.to_csv('/content/ans3.csv')\n",
        "\n",
        "#np.savetxt('/content/ans.csv', pre_ans, delimiter=',' , fmt='%d')\n",
        "\n",
        "\n",
        "#予測値の表示\n",
        "#survive_pred = model.predict(X_test,num_iteration=model.best_iteration)\n",
        "\n",
        "survive_pred = pred\n",
        "print(pred )\n",
        "\n",
        "\n",
        "# 予測結果を変換\n",
        "y_pred_fin=[]\n",
        "for x in survive_pred:\n",
        "    y_pred_fin.append(round(x))\n",
        "\n",
        "#混合行列作成\n",
        "print(confusion_matrix(y_test, y_pred_fin))\n",
        "\n",
        "# 評価\n",
        "print(accuracy_score(y_test, y_pred_fin))\n",
        "\n",
        "\n",
        "\n",
        "#特徴量重要度を算出するだけの部分なので、採用する変数を決定してしまったら用済み\n",
        "## 特徴量重要度の算出 (データフレームで取得)\n",
        "cols = list(traindata.drop('Survived',axis=1).columns)       \n",
        "\n",
        "# 特徴量重要度の算出方法 'gain'(推奨) : トレーニングデータの損失の減少量を評価\n",
        "f_importance = np.array(model.feature_importance(importance_type='gain')) # 特徴量重要度の算出 //\n",
        "f_importance = f_importance / np.sum(f_importance) # 正規化(必要ない場合はコメントアウト)\n",
        "df_importance = pd.DataFrame({'feature':cols, 'importance':f_importance})\n",
        "df_importance = df_importance.sort_values('importance', ascending=False) # 降順ソート\n",
        "display(df_importance)\n",
        "plot_feature_importance(df_importance)\n"
      ],
      "metadata": {
        "id": "SWrKY_Opsbd6",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title タイタニック　アンサンブルver\n",
        "\n",
        "#@title 国税調査からの年収予測 Part2\n",
        "\n",
        "#import\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import log_loss  #対数損失のインポート\n",
        "from sklearn.model_selection import KFold\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import lightgbm as lgb\n",
        "\n",
        "\n",
        "#データ読み込み\n",
        "pretrain = pd.read_csv(\"/content/train.csv\") # 学習用データ\n",
        "pretest = pd.read_csv(\"/content/test.csv\")   # 評価用データ\n",
        "sample_submit = pd.read_csv(\"/content/sample_submit.csv\", header=None) # 応募用サンプルファイル\n",
        "\n",
        "#訓練データに改変\n",
        "train_x = pretrain.drop(['Y'],axis=1)\n",
        "train_y = pretrain['Y']\n",
        "test_x  = pretest.copy()\n",
        "\n",
        "#LabelEncoder\n",
        "for c in ['workclass','education','marital-status','occupation','relationship','race','sex','native-country']:\n",
        "  le = LabelEncoder()\n",
        "  le.fit(train_x[c].fillna('NA'))\n",
        "  \n",
        "  train_x[c] = le.transform(train_x[c].fillna('NA'))\n",
        "  test_x[c]  = le.transform(test_x[c].fillna('NA'))\n",
        "\n",
        "\n",
        "\n",
        "model = XGBClassifier(n_estimatiors=20,random_state=7)\n",
        "model.fit(train_x,train_y)\n",
        "\n",
        "pred_xgb = model.predict_proba(test_x)[:,1]\n",
        "\n",
        "\n",
        "# -----------------------------------\n",
        "# ロジスティック回帰用の特徴量の作成\n",
        "# -----------------------------------\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# 元データをコピーする\n",
        "train_x2 = pretrain.drop(['Y'], axis=1)\n",
        "test_x2 = pretest.copy()\n",
        "\n",
        "# one-hot encodingを行う\n",
        "cat_cols = ['workclass','education','marital-status','occupation','relationship','race','sex','native-country']\n",
        "ohe = OneHotEncoder(categories='auto', sparse=False)\n",
        "ohe.fit(train_x2[cat_cols].fillna('NA'))\n",
        "\n",
        "# one-hot encodingのダミー変数の列名を作成する\n",
        "ohe_columns = []\n",
        "for i, c in enumerate(cat_cols):\n",
        "    ohe_columns += [f'{c}_{v}' for v in ohe.categories_[i]]\n",
        "\n",
        "# one-hot encodingによる変換を行う\n",
        "ohe_train_x2 = pd.DataFrame(ohe.transform(train_x2[cat_cols].fillna('NA')), columns=ohe_columns)\n",
        "ohe_test_x2 = pd.DataFrame(ohe.transform(test_x2[cat_cols].fillna('NA')), columns=ohe_columns)\n",
        "\n",
        "# one-hot encoding済みの変数を除外する\n",
        "train_x2 = train_x2.drop(cat_cols, axis=1)\n",
        "test_x2 = test_x2.drop(cat_cols, axis=1)\n",
        "\n",
        "# one-hot encodingで変換された変数を結合する\n",
        "train_x2 = pd.concat([train_x2, ohe_train_x2], axis=1)\n",
        "test_x2 = pd.concat([test_x2, ohe_test_x2], axis=1)\n",
        "\n",
        "\n",
        "# -----------------------------------\n",
        "# アンサンブル\n",
        "# -----------------------------------\n",
        "\n",
        "# ロジスティック回帰モデル\n",
        "# xgboostモデルとは異なる特徴量を入れる必要があるので、別途train_x2, test_x2を作成した\n",
        "model_lr = LogisticRegression(solver='lbfgs', max_iter=300)\n",
        "model_lr.fit(train_x2, train_y)\n",
        "pred_lr = model_lr.predict_proba(test_x2)[:, 1]\n",
        "\n",
        "# 予測値の加重平均をとる\n",
        "pred = pred_xgb * 0.8 + pred_lr * 0.2\n",
        "pred_label = np.where(pred > 0.5, 1, 0)\n",
        "\n",
        "submission = pd.DataFrame({'index':test_x['index'], 'Y':pred_label})\n",
        "submission.to_csv('/content/ans2.csv',index=False)\n"
      ],
      "metadata": {
        "id": "vMln3TuRaQH9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Player Contact Detection　締め切り2023/03/01\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "import lightgbm as lgb\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split # データセット分割用\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pylab import rcParams\n",
        "from sklearn.metrics import confusion_matrix\n",
        "np.set_printoptions(suppress=True)#指数表記禁止\n",
        "\n",
        "import IPython\n",
        "def display(*dfs, head=True):\n",
        "    for df in dfs:\n",
        "        IPython.display.display(df.head() if head else df)\n",
        "\n",
        "# 特徴量重要度を棒グラフでプロットする関数 \n",
        "def plot_feature_importance(df): \n",
        "  n_features = len(df)                              # 特徴量数(説明変数の個数) \n",
        "  df_plot = df.sort_values('importance')            # df_importanceをプロット用に特徴量重要度を昇順ソート \n",
        "  f_importance_plot = df_plot['importance'].values  # 特徴量重要度の取得 \n",
        "  plt.barh(range(n_features), f_importance_plot, align='center') \n",
        "  cols_plot = df_plot['feature'].values             # 特徴量の取得 \n",
        "  plt.yticks(np.arange(n_features), cols_plot)      # x軸,y軸の値の設定\n",
        "  plt.xlabel('Feature importance')                  # x軸のタイトル\n",
        "  plt.ylabel('Feature')                             # y軸のタイトル\n"
      ],
      "metadata": {
        "id": "s8nMZYFu45Y6",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Playground Series - Season 3, Episode 5 アンサンブルver\n",
        "\n",
        "#import\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import log_loss  #対数損失のインポート\n",
        "from sklearn.model_selection import KFold\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import lightgbm as lgb\n",
        "\n",
        "\n",
        "#データ読み込み\n",
        "pretrain = pd.read_csv(\"/content/train.csv\") # 学習用データ\n",
        "pretest = pd.read_csv(\"/content/test.csv\")   # 評価用データ\n",
        "#sample_submit = pd.read_csv(\"/content/sample_submit.csv\", header=None) # 応募用サンプルファイル\n",
        "\n",
        "#訓練データに改変\n",
        "train_x = pretrain.drop(['quality'],axis=1)\n",
        "train_y = pretrain['quality']\n",
        "test_x  = pretest.copy()\n",
        "\n",
        "#LabelEncoder\n",
        "#for c in ['fixed acidity', 'volatile acidity','citric acid','residual sugar','']:\n",
        " # le = LabelEncoder()\n",
        "  #le.fit(train_x[c].fillna('NA'))\n",
        "  \n",
        "  #train_x[c] = le.transform(train_x[c].fillna('NA'))\n",
        "  #test_x[c]  = le.transform(test_x[c].fillna('NA'))\n",
        "\n",
        "\n",
        "#intに変換\n",
        "#for c in ['fixed acidity', 'volatile acidity','citric acid','residual sugar','chlorides','free sulfur dioxide','total sulfur dioxide','density','pH','sulphates','alcohol']:\n",
        " # le = LabelEncoder()\n",
        "  #le.fit(train_x[c].fillna('NA'))\n",
        "  \n",
        "  #train_x[c] = le.transform(train_x[c].fillna('NA'))\n",
        "  #test_x[c]  = le.transform(test_x[c].fillna('NA'))\n",
        "\n",
        "\n",
        "model = XGBClassifier(n_estimatiors=20,random_state=7)\n",
        "model.fit(train_x,train_y)\n",
        "\n",
        "pred_xgb = model.predict_proba(test_x)[:,1]\n",
        "\n",
        "\n",
        "# -----------------------------------\n",
        "# ロジスティック回帰用の特徴量の作成\n",
        "# -----------------------------------\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# 元データをコピーする\n",
        "train_x2 = pretrain.drop(['quality'], axis=1)\n",
        "test_x2 = pretest.copy()\n",
        "\n",
        "'''\n",
        "# one-hot encodingを行う\n",
        "cat_cols = ['fixed acidity', 'volatile acidity','citric acid','residual sugar','chlorides','free sulfur dioxide','total sulfur dioxide','density','pH','sulphates','alcohol']\n",
        "ohe = OneHotEncoder(categories='auto', sparse=False)\n",
        "ohe.fit(train_x2[cat_cols].fillna('NA'))\n",
        "\n",
        "# one-hot encodingのダミー変数の列名を作成する\n",
        "ohe_columns = []\n",
        "for i, c in enumerate(cat_cols):\n",
        "    ohe_columns += [f'{c}_{v}' for v in ohe.categories_[i]]\n",
        "\n",
        "# one-hot encodingによる変換を行う\n",
        "ohe_train_x2 = pd.DataFrame(ohe.transform(train_x2[cat_cols].fillna('NA')), columns=ohe_columns)\n",
        "ohe_test_x2 = pd.DataFrame(ohe.transform(test_x2[cat_cols].fillna('NA')), columns=ohe_columns)\n",
        "\n",
        "# one-hot encoding済みの変数を除外する\n",
        "train_x2 = train_x2.drop(cat_cols, axis=1)\n",
        "test_x2 = test_x2.drop(cat_cols, axis=1)\n",
        "\n",
        "# one-hot encodingで変換された変数を結合する\n",
        "train_x2 = pd.concat([train_x2, ohe_train_x2], axis=1)\n",
        "test_x2 = pd.concat([test_x2, ohe_test_x2], axis=1)\n",
        "'''\n",
        "\n",
        "# -----------------------------------\n",
        "# アンサンブル\n",
        "# -----------------------------------\n",
        "\n",
        "# ロジスティック回帰モデル\n",
        "# xgboostモデルとは異なる特徴量を入れる必要があるので、別途train_x2, test_x2を作成した\n",
        "model_lr = LogisticRegression(solver='lbfgs', max_iter=300)\n",
        "model_lr.fit(train_x2, train_y)\n",
        "pred_lr = model_lr.predict_proba(test_x2)[:, 1]\n",
        "\n",
        "# 予測値の加重平均をとる\n",
        "pred = (pred_xgb * 0.8 + pred_lr * 0.2)*100\n",
        "pred_label = pred.astype('int8')\n",
        "\n",
        "submission = pd.DataFrame({'Id':test_x['Id'], 'quality':pred_label})\n",
        "submission.to_csv('/content/ans2.csv',index=False)\n"
      ],
      "metadata": {
        "id": "9ByKsqFrJt8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f6a87f4-93fa-4625-9847-f4ca589e8f21",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Playground Series Season 3, Episode 5\n",
        "import csv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "import lightgbm as lgb\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split # データセット分割用\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pylab import rcParams\n",
        "from sklearn.metrics import confusion_matrix\n",
        "np.set_printoptions(suppress=True)#指数表記禁止\n",
        "\n",
        "import IPython\n",
        "def display(*dfs, head=True):\n",
        "    for df in dfs:\n",
        "        IPython.display.display(df.head() if head else df)\n",
        "\n",
        "# 特徴量重要度を棒グラフでプロットする関数 \n",
        "def plot_feature_importance(df): \n",
        "  n_features = len(df)                              # 特徴量数(説明変数の個数) \n",
        "  df_plot = df.sort_values('importance')            # df_importanceをプロット用に特徴量重要度を昇順ソート \n",
        "  f_importance_plot = df_plot['importance'].values  # 特徴量重要度の取得 \n",
        "  plt.barh(range(n_features), f_importance_plot, align='center') \n",
        "  cols_plot = df_plot['feature'].values             # 特徴量の取得 \n",
        "  plt.yticks(np.arange(n_features), cols_plot)      # x軸,y軸の値の設定\n",
        "  plt.xlabel('Feature importance')                  # x軸のタイトル\n",
        "  plt.ylabel('Feature')                             # y軸のタイトル\n",
        "\n",
        "\n",
        "#データよみこみ\n",
        "pretest  = pd.read_csv(filepath_or_buffer=\"/content/test.csv\")\n",
        "pretrain = pd.read_csv(filepath_or_buffer=\"/content/train.csv\")\n",
        "\n",
        "#扱いづらいデータを削除\n",
        "traindata = pretrain.drop(['Id','pH','chlorides','fixed acidity'], axis=1)\n",
        "testdata  = pretest.drop(['Id','pH','chlorides','fixed acidity'], axis=1)\n",
        "\n",
        "#欠損値処理\n",
        "#np.nan_to_num(traindata['Age'], copy=False)\n",
        "#np.nan_to_num(testdata['Age'], copy=False)\n",
        "\n",
        "#male or female\n",
        "#SEX_le = LabelEncoder()\n",
        "#traindata['Sex'] = SEX_le.fit_transform(traindata['Sex'])\n",
        "#testdata['Sex']  = SEX_le.fit_transform(testdata['Sex'])\n",
        "\n",
        "#embarked\n",
        "#eb_le = LabelEncoder()\n",
        "#traindata['Embarked'] = eb_le.fit_transform(traindata['Embarked'])\n",
        "#testdata['Embarked']  = eb_le.fit_transform(testdata['Embarked'])\n",
        "\n",
        "#Ageを整数型に変換\n",
        "#traindata['Age'] = traindata['Age'].astype('int')\n",
        "#testdata['Age']  = testdata['Age'].astype('int')\n",
        "\n",
        "\n",
        "# 説明変数,目的変数, テストケース分割\n",
        "X = traindata.drop('quality',axis=1).values # 説明変数\n",
        "y = traindata['quality'].values             # 目的変数\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.10, random_state=2) \n",
        "#↑学習用データ：テストデータ=8:2になるようにしている\n",
        "\n",
        "\n",
        "\n",
        "# 学習に使用するデータを設定\n",
        "putilgb_train = lgb.Dataset(X_train, y_train)\n",
        "putilgb_eval = lgb.Dataset(X_test, y_test, reference=putilgb_train) \n",
        "\n",
        "\n",
        "# LightGBM parameters\n",
        "params = {\n",
        "        'task': 'train',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'objective':'multiclass',\n",
        "        'metric': {'multi_logloss'}, # 評価指標 : rsme(平均二乗誤差の平方根) \n",
        "        'num_class': 9,\n",
        "        'learning_rate': 0.01,\n",
        "        'num_iterations':500,  \n",
        "        'num_leaves':30,\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# モデルの学習\n",
        "model = lgb.train(params,\n",
        "                  train_set=putilgb_train, # トレーニングデータの指定\n",
        "                  num_boost_round=1000,\n",
        "                  valid_sets=putilgb_eval, # 検証データの指定\n",
        "                  early_stopping_rounds=20,\n",
        "                  verbose_eval=0\n",
        "                  )\n",
        "\n",
        "\n",
        "\n",
        "pred = model.predict(testdata)\n",
        "\n",
        "pred   = pred.astype('int')\n",
        "\n",
        "\n",
        "submission = pd.DataFrame({'Id':pretest['Id'], 'quality':pred})\n",
        "submission.to_csv('/content/ans3.csv',index=False)\n",
        "\n",
        "\n",
        "#pre_ans = np.stack([pretest[\"Id\"], pred], 1)\n",
        "#print(pre_ans)\n",
        "\n",
        "#ans = pd.DataFrame(pre_ans, columns=['Id', 'quality'])\n",
        "#print(ans)\n",
        "\n",
        "\n",
        "#ans = ans.drop(ans.columns[[0]] , axis=1)\n",
        "#ans.to_csv('/content/ans2.csv')\n",
        "\n",
        "#np.savetxt('/content/ans.csv', pre_ans, delimiter=',' , fmt='%d')\n",
        "\n"
      ],
      "metadata": {
        "id": "sVgqzl1vTMAT",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Playground Series - Season 3, Episode 5 Part3\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import xgboost as xgb \n",
        "import lightgbm as lgbm\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "path = '/content/'\n",
        "train = pd.read_csv(path +'train.csv').drop('Id',axis=1)\n",
        "target = 'quality'\n",
        "features = [c for c in train.columns if c not in ['id','Time', target]]\n",
        "\n",
        "#combine with original training set\n",
        "orig_train = pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\n",
        "orig_train = orig_train[~orig_train.duplicated()]\n",
        "train = pd.concat([train,orig_train]).reset_index(drop=True)\n",
        "train['split']= 'train'\n",
        "test = pd.read_csv(path +'test.csv').drop('Id',axis=1)\n",
        "test['split'] = 'test'\n",
        "data = pd.concat([train,test]).reset_index(drop=True)\n",
        "sub = pd.read_csv(path +'sample_submission.csv')"
      ],
      "metadata": {
        "id": "hbbyIVgprmDi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}